<a id="readme-top"></a>
<h1 align="center">Sequence to Sequence Models</h1>
<p align="center"> 
  <img src="/3%20Aug'23%20Attention%20Based%20Model/images/encoder_decoder.png" alt="Sequence" >
</p>
<h2 id="intro">Introduction</h2>
<p align="justify"> 
  Sequence-to-sequence (Seq2Seq) models are a type of neural network architecture used in NLP for tasks that involve input and output sequences of varying lengths.
</p>
<p align="justify"> 
    The basic idea of a Seq2Seq model is to take an input sequence (e.g., a sentence) and map it to a variable-length output sequence (e.g., a translation in a different language). This is accomplished through two main components: an encoder and a decoder.
</p>
<p align="justify"> 
    The encoder takes the input sequence and produces a fixed-size representation (a vector) that summarizes the information in the input sequence. The decoder takes this vector as input and generates the output sequence one token at a time, using the context provided by the encoder to make predictions about the next token in the sequence
</p>

<p align="right">(<a href="#readme-top">back to top</a>)</p>

![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png)

<img src="/3%20Aug'23%20Attention%20Based%20Model/images/enc_de.png" alt="Sequence" >

<h2 id="intro">ENCODER</h2>
  <ol>
    <li> Converts the input words to corresponding hidden vectors</li>
    <li>Each vector represents the current word and the context of the word</li>
    <li>The encoder takes the input sequence, one token at a time, and uses an RNN or transformer to update its hidden state, which summarizes the information in the input sequence</li>
    <li>The final hidden state of the encoder is then passed as the context vector to the decoder.</li>
  </ol>

<h2 id="intro">DECODER</h2>
 <ol>
    <li>It takes as input the hidden vector generated by the encoder, its own hidden states, and the current word to produce the next hidden vector and finally predict the next word</li>
    <li>The decoder uses the context vector and an initial hidden state to generate the output sequence, one token at a time</li>
    <li>At each time step, the decoder uses the current hidden state, the context vector and the previous output token to generate a probability distribution over the possible next tokens. </li>
    <li>The token with the highest probability is then chosen as the output, and the process continues until the end of the output sequence is reached.</li>
  </ol>

<p align="right">(<a href="#readme-top">back to top</a>)</p>

![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png)
<h2 id="pros">Advantages of seq2seq Models</h2>

1. <b>Flexibility</b>
   + `Seq2Seq models can handle a wide range of tasks such as machine translation, text summarization, and image captioning, as well as variable-length input and output sequences`
2. <b>Handling Sequential Data</b>
   + `Seq2Seq models are well-suited for tasks that involve sequential data such as natural language, speech, and time series data.`
3. **Handling Context**
    + `The encoder-decoder architecture of Seq2Seq models allows the model to capture the context of the input sequence and use it to generate the output sequence.`
4. **Attention Mechanism**
   + `Using attention mechanisms allows the model to focus on specific parts of the input sequence when generating the output, which can improve performance for long input sequences`
   
<p align="right">(<a href="#readme-top">back to top</a>)</p>

![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png)
<h2 id="pros">Disadvantages of seq2seq Models</h2>

1. <b> Computationally Expensive</b>
   + `Seq2Seq models require significant computational resources to train and can be difficult to optimize.`
2. ** Limited Interpretability**
   + `The internal workings of Seq2Seq models can be difficult to interpret, which can make it challenging to understand why the model is making certain decisions`
3. **Overfitting**
   + `Seq2Seq models can overfit the training data if they are not properly regularized, which can lead to poor performance on new data.`
4. **Handling Long input Sequences**
   + `Seq2Seq models can have difficulty handling input sequences that are very long, as the context vector may not be able to capture all the information in the input sequence.`
<br>


<p align="right">(<a href="#readme-top">back to top</a>)</p>