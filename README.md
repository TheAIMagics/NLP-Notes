<p align="center">
    <img src="/word_cloud.png" height="300px">
</p>

# NLP Documentation and Materials
<table>
<tr>
<td>
  This repository contains tutorials covering understanding NLP from basic to Advance.
</td>
</tr>
</table>

## Tutorials



* 1 - [NLP Introduction](https://github.com/iNeuron-Pvt-Ltd/NLP-Documentation-and-Material/tree/main/22%20July'23%20NLP%20Introduction) 

    This first tutorial we will cover the basic topics of NLP like Computational Linguistics then we will try to answer few questions in this context like History of NLP & its uses. Then at will try to understand reason for using NLP.

* 2 - [Text Processing For NLP](https://github.com/iNeuron-Pvt-Ltd/NLP-Documentation-and-Material/blob/main/23%20July'23%20Text%20Processing%20For%20NLP/Text%20Processing%20For%20NLP.md) 

    This tutorial will focus mainly on basic text pre processing steps like Text cleaning,Tokenization, Stemming, Lemmatization, Stemming vs Lemmatization,Part-of-speech tagging and Named entity recognition.

* 3 - [Advance Text Processing for NLP](https://github.com/iNeuron-Pvt-Ltd/NLP-Documentation-and-Material/blob/main/25%20July'23%20Text%20Processing%20For%20NLP/July'25%20Text%20Processing%20For%20NLP.ipynb) 

    Next, we learn about few advance text pre processing steps undes which we will try to answer few questions like what is Embedding? what is Word2Vec and learn its type CBOW and skipgram. Then gain basic understanding of Gensim.

* 4 - [Useful NLP Libraries](https://github.com/iNeuron-Pvt-Ltd/NLP-Documentation-and-Material/blob/main/26%20July'23%20Useful%20NLP%20Libraries/NLTK%20Library.ipynb)

    In this notebook, we learn our first fundamental library named NLTK in which we will perform Sentiment Analysis with scikit-learn(IMDb Reviews).

* 5 - [Useful NLP Libraries](https://github.com/iNeuron-Pvt-Ltd/NLP-Documentation-and-Material/blob/main/27%20July'23%20Useful%20NLP%20Libraries/TextBlob%20Library.ipynb)

    In this notebook, we learn our second fundamental library named TextBlob in which we will work on spell checker using TextBlob library. 

* 6 - [NLP Networks - RNN](https://github.com/iNeuron-Pvt-Ltd/NLP-Documentation-and-Material/tree/main/29%20July'23%20NLP%20Networks) 

    This notebook covers the basic of Recurrent Neural Network (RNN). Then We will build a deep learning model to detect sentiment (i.e. detect if a sentence is positive or negative) using PyTorch and TorchText. We will learn basics of torchtext such as Field, LabelField, BucketIterator etc.

* 7 - [NLP Networks - LSTM](https://github.com/iNeuron-Pvt-Ltd/NLP-Documentation-and-Material/tree/main/30%20July'23%20NLP%20Networks) 

    This notebook covers the basic of Long Short-Term Memory(LSTM). How LSTM architecture works with all gates and states defined. Then We will build a deep learning model to detect sentiment (i.e. detect if a sentence is positive or negative) using PyTorch and TorchText. We will learn basics of torchtext such as Field, LabelField, BucketIterator etc. How to prepare data using torchtext and how do we prepare iterators ?

* 8 - [NLP Networks - BiLSTM](https://github.com/iNeuron-Pvt-Ltd/NLP-Documentation-and-Material/tree/main/31%20July'23%20NLP%20Networks)

    This markdaown file contains basic theory behind BiLSTM

* 9 - [NLP Networks - Stacked LSTM](https://github.com/iNeuron-Pvt-Ltd/NLP-Documentation-and-Material/tree/main/31%20July'23%20NLP%20Networks)

    This markdaown file contains basic theory behind Stacked LSTM

* 10 - [NLP Networks - GRU](https://github.com/iNeuron-Pvt-Ltd/NLP-Documentation-and-Material/tree/main/2%20Aug'23%20NLP%20Networks)

    This notebook covers the basic of Introduction to Gated Recurrent Unit (GRU). Then we will learn what is the basic difference between LSTM and GRU. We have used Sequence2Sequence common model ie. Encoder and Decoder models to translate German sequence to English Sequence.

* 11 - [Attention Based Model - Encoder-Decoder](https://github.com/iNeuron-Pvt-Ltd/NLP-Documentation/blob/main/3%20Aug'23%20Attention%20Based%20Model/encoder_decoder.md)

    In markdown file we gain basic understanding of seq2seq networks using encoder-decoder models. Then We will learn to implement a sequence to sequence model which can tranlate German(encoder) to English(decoder) Language using  Multi30k dataset in Pytorch, then how to use torchtext to do all of the heavy lifting with regards to text processing.
