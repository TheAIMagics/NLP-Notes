{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a790c165",
   "metadata": {},
   "source": [
    "### Objective :  \n",
    "- With help of simple text classification task based on AG_NEWS sample dataset we will learn text procesing task.\n",
    "\n",
    "### About Dataset:\n",
    "- Main Objective of AG_NEWS dataset is to classify news headlines into one of 4 categories:***World, Sports, Business and Sci/Tech***\n",
    "- This dataset is built from PyTorch's torchtext module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfaac4b",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47462a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\TorchText\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import os\n",
    "import collections\n",
    "os.makedirs('./data',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3695ca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset,test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca23007",
   "metadata": {},
   "source": [
    "#### train_dataset & test_dataset contain iterators that returns pair of label and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20edb80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torchtext.data.datasets_utils._RawTextIterableDataset at 0x21d38c55a00>,\n",
       " <torchtext.data.datasets_utils._RawTextIterableDataset at 0x21d38ca4700>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset,test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beea4df5",
   "metadata": {},
   "source": [
    "#### Display top 3 news "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a73e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Sci/Tech> <-> Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "\n",
      "<Sci/Tech> <-> Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
      "\n",
      "<Sci/Tech> <-> Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, tuple_data in zip(range(3),train_dataset):\n",
    "    print(f\"<{classes[tuple_data[0]]}> <-> {tuple_data[1]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebd7b13",
   "metadata": {},
   "source": [
    "#### Converting train_dataset, test_dataset iterators to List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4041469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = list(train_dataset)\n",
    "test_dataset = list(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe957da",
   "metadata": {},
   "source": [
    "#### First step is to convert text to tokens - tokenization.\n",
    "\n",
    "#### `Tokenization` : \n",
    "- The process of converting text into a sequence of tokens is called tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75816250",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6acd8163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token list:\n",
      "['iraq', 'halts', 'oil', 'exports', 'from', 'main', 'southern', 'pipeline', '(', 'reuters', ')', 'reuters', '-', 'authorities', 'have', 'halted', 'oil', 'export\\\\flows', 'from', 'the', 'main', 'pipeline', 'in', 'southern', 'iraq', 'after\\\\intelligence', 'showed', 'a', 'rebel', 'militia', 'could', 'strike\\\\infrastructure', ',', 'an', 'oil', 'official', 'said', 'on', 'saturday', '.']\n"
     ]
    }
   ],
   "source": [
    "ex_sentence = train_dataset[0][1]\n",
    "tokens = tokenizer(ex_sentence)\n",
    "print(f'\\nToken list:\\n{tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ceb36c",
   "metadata": {},
   "source": [
    "#### Next step to convert text to numbers is to build Vocalulary for which we will use counter object\n",
    "#### `Vectorization` :\n",
    "- The process of converting each token into number that can be represented as tensors which can be feed to neural nework is called vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3315b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "for (label,line) in train_dataset:\n",
    "    #This step essentially counts the number of occurrences of each token in the entire training dataset.\n",
    "    counter.update(tokenizer(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fd3c6c",
   "metadata": {},
   "source": [
    "#### Create a Vocab object that would help us deal with vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30e5231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = torchtext.vocab.Vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccb9420",
   "metadata": {},
   "source": [
    "#### Using vocabulary object, we can easily encode our tokenized string into a set of numbers\n",
    "`vocab.stoi` allows us to convert from a string representation into numbers (the name stoi stands for \"from string to integers). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72ce6c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 95809\n",
      "[71, 7377, 59, 1811, 30, 906, 538, 2847, 14, 28, 15, 28, 16, 839, 40, 4979, 59, 68867, 30, 3, 906, 2847, 8, 538, 71, 58871, 704, 6, 913, 2521, 94, 89166, 4, 31, 59, 294, 27, 11, 115, 2]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(\"Vocab Size\", vocab_size)\n",
    "def encode(x):\n",
    "    return [vocab.stoi[s] for s in tokenizer(x)]\n",
    "vector = encode(ex_sentence)\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10417d53",
   "metadata": {},
   "source": [
    "#### Limitations of word tokenization:\n",
    "- 1. Ambiguity\n",
    "- 2. Compound words\n",
    "- 3. Punctuation - Tokenizing words based solely on whitespace or punctuation marks can also be problematic. \n",
    "- 4. Out-of-vocabulary words\n",
    "- 5. Languages without whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58588f86",
   "metadata": {},
   "source": [
    "#### To overcome limitations of word tokenization we use N-gram representations\n",
    "- N-grams are contiguous sequences of N words from a text\n",
    "- N-gram representations are a commonly used technique for capturing the context and meaning of words in NLP tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaf83d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import ngrams_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88d56a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram_counter = collections.Counter()\n",
    "for (label,line) in train_dataset:\n",
    "    bi_gram_counter.update(ngrams_iterator(tokenizer(line), ngrams=2))\n",
    "bi_gram_vocab = torchtext.vocab.Vocab(bi_gram_counter, min_freq=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babe92eb",
   "metadata": {},
   "source": [
    "#### Limitations of  N-gram representations:\n",
    "- 1. Fixed window size\n",
    "- 2. Sparsity\n",
    "- 3. Limited context \n",
    "- 4. Fixed vocabulary\n",
    "- 5. Curse of dimensionality\n",
    "\n",
    "In practice, n-gram vocabulary size is still too high to represent words as one-hot vectors, and thus we need to combine this representation with some dimensionality reduction techniques, such as *embeddings*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchtext",
   "language": "python",
   "name": "torchtext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
